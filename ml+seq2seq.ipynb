{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq在机器翻译上的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocab of english and chinese\n",
    "import codecs\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "\n",
    "# DARE_TYPE = \"english\"  or\n",
    "# DARE_TYPE = \"chinese\"\n",
    "\n",
    "def get_vocab(DARE_TYPE):\n",
    "    if DARE_TYPE == \"chinese\":\n",
    "        RAW_DATA = \"./train.txt.zh\"\n",
    "        VOCAB_OUTPUT = \"zh.vocab\"\n",
    "        VOCAB_SIZE = 4000\n",
    "    elif DARE_TYPE == \"english\":\n",
    "        RAW_DATA = \"./train.txt.en\"\n",
    "        VOCAB_OUTPUT = \"en.vocab\"\n",
    "        VOCAB_SIZE = 10000\n",
    "\n",
    "    counter = collections.Counter()\n",
    "\n",
    "    with codecs.open(RAW_DATA,\"r\",\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split():\n",
    "                counter[word] += 1\n",
    "\n",
    "    sorted_word_to_cnt = sorted(counter.items(),key=itemgetter(1),reverse = True)\n",
    "\n",
    "    sorted_word_list = [x[0] for x in sorted_word_to_cnt]\n",
    "\n",
    "    sorted_word_list = [\"<unk>\",\"<sos>\",\"<eos>\"] + sorted_word_list\n",
    "\n",
    "    if len(sorted_word_list) > VOCAB_SIZE:\n",
    "        sorted_word_list = sorted_word_list[:VOCAB_SIZE]\n",
    "\n",
    "    with codecs.open(VOCAB_OUTPUT,'w','utf-8') as file_output:\n",
    "        for word in sorted_word_list:\n",
    "            file_output.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences id\n",
    "DATA_TYPE = \"english\"\n",
    "\n",
    "if DATA_TYPE == \"chinese\":  # 翻译语料的中文部分\n",
    "    RAW_DATA = \"./train.txt.zh\"\n",
    "    VOCAB = \"zh.vocab\"\n",
    "    OUTPUT_DATA = \"train.zh\"\n",
    "elif DATA_TYPE == \"english\":  # 翻译语料的英文部分\n",
    "    RAW_DATA = \"./train.txt.en\"\n",
    "    VOCAB = \"en.vocab\"\n",
    "    OUTPUT_DATA = \"train.en\"\n",
    "\n",
    "with codecs.open(VOCAB,'r','utf-8') as f_vocab:\n",
    "    vocab = [w.strip() for w in f_vocab.readlines()]\n",
    "\n",
    "word_to_id = {k: v for (k,v) in zip(vocab,range(len(vocab)))}\n",
    "\n",
    "def get_id(word):\n",
    "    return word_to_id[word] if word in word_to_id else word_to_id['<unk>']\n",
    "\n",
    "fin = codecs.open(RAW_DATA,'r','utf-8')\n",
    "fout = codecs.open(OUTPUT_DATA,'w','utf-8')\n",
    "\n",
    "for line in fin:\n",
    "    words = line.strip().split()+ [\"<eos>\"]\n",
    "    out_line = ' '.join([str(get_id(w)) for w in words]) + '\\n'\n",
    "    fout.write(out_line)\n",
    "\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id(\"<sos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#padding and batching\n",
    "import tensorflow as tf\n",
    "\n",
    "MAX_LEN = 50\n",
    "SOS_ID = 1\n",
    "\n",
    "def MakeDataset(file_path):\n",
    "    dataset = tf.data.TextLineDataset(file_path)\n",
    "    dataset = dataset.map(lambda string:tf.string_split([string]).values)\n",
    "    dataset = dataset.map(lambda string:tf.string_to_number(string,tf.int32))\n",
    "    dataset = dataset.map(lambda x: (x, tf.size(x)))\n",
    "    return dataset\n",
    "\n",
    "def MakeSrcTrgDataset(src_path,trg_path,batch_size):\n",
    "    src_data = MakeDataset(src_path)\n",
    "    trg_data = MakeDataset(trg_path)\n",
    "    dataset = tf.data.Dataset.zip((src_data, trg_data))\n",
    "\n",
    "    def FilterLength(src_tuple,trg_tuple):\n",
    "        ((src_input,src_len),(trg_label,trg_len)) = (src_tuple,trg_tuple)\n",
    "        src_len_ok = tf.logical_and(tf.greater(src_len,1),tf.less_equal(src_len,MAX_LEN))\n",
    "        trg_len_ok = tf.logical_and(tf.greater(trg_len,1),tf.less_equal(trg_len, MAX_LEN))\n",
    "        return tf.logical_and(src_len_ok,trg_len_ok)\n",
    "\n",
    "    dataset = dataset.filter(FilterLength)\n",
    "\n",
    "    def MakeTrgInput(src_tuple,trg_tuple):\n",
    "        ((src_input,src_len),(trg_label, trg_len)) = (src_tuple, trg_tuple)\n",
    "        trg_input = tf.concat([[SOS_ID], trg_label[:-1]], axis=0)\n",
    "        return ((src_input, src_len), (trg_input, trg_label, trg_len))\n",
    "\n",
    "    dataset = dataset.map(MakeTrgInput)\n",
    "\n",
    "    dataset = dataset.shuffle(10000)\n",
    "\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]),    # 源句子是长度未知的向量\n",
    "         tf.TensorShape([])),       # 源句子长度是单个数字\n",
    "        (tf.TensorShape([None]),    # 目标句子(解码器输入)是长度未知的向量\n",
    "         tf.TensorShape([None]),    # 目标句子(解码器目标输出)是长度未知的向量\n",
    "         tf.TensorShape([]))        # 目标句子长度(输出)是单个数字\n",
    "    )\n",
    "\n",
    "    batched_dataset = dataset.padded_batch(batch_size,padded_shapes)\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(object):\n",
    "    def __init__(self):\n",
    "        self.enc_cell = tf.nn.rnn_cell.MutiRNNCell(\n",
    "        [tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n",
    "        self.dec_cell = tf.nn.rnn_cell.MutiRNNCell(\n",
    "        [tf.nn.rnn_cell.LSTMCell(HIDDEN_SIZE) for _ in range(NUM_LAYERS)])\n",
    "        self.src_embedding = tf.get_variable('src_emb', [SRC_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        self.trg_embedding = tf.get_variable('trg_emb', [TRG_VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        if SHARE_EMB_AND_SOFTMAX:\n",
    "            self.softmax_weight = tf.transpose(self.trg_embedding)\n",
    "        else:\n",
    "            self.softmax_weight = tf.get_variable(\"weight\",[HIDDEN_SIZE,TRG_VOCAB])\n",
    "        self.softmax_bias = tf.get_variable('softmax_loss',[TRG_VOCAB_SIZE])\n",
    "        \n",
    "    def forward(self,src_input,src_size,trg_input,trg_label,trg_size):\n",
    "        batch_size = tf.shape(src_input)[0]\n",
    "        src_emb = tf.nn.embedding_lookup(self.src_embedding,src_input)\n",
    "        trg_emb = tf.nn.embedding_lookup(self.trg_embedding,trg_input)\n",
    "        src_emb = tf.nn.dropout(src_emb,KEEP_PROB)\n",
    "        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)\n",
    "        # 编码器读取源句子每个位置的词向量，输出最后一步的隐藏状态enc_state\n",
    "        with tf.variable_scope('encoder'):\n",
    "            enc_outputs,enc_state = tf.nn.dynamic_rnn(self.enc_cell,src_emb,src_size, dtype=tf.float32)\n",
    "        output = tf.reshape(dec_outputs, [-1, HIDDEN_SIZE])\n",
    "        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(trg_label, [-1]), logits=logits)\n",
    "        label_weights = tf.sequence_mask(trg_size, maxlen=tf.shape(trg_label)[1], dtype=tf.float32)\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "        cost = tf.reduce_sum(loss * label_weights)\n",
    "        cost_per_token = cost / tf.reduce_sum(label_weights)\n",
    "        # 定义反向传播操作\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        # 控制梯度大小，定义优化方法和训练步骤\n",
    "        # 算出每个需要更新的值的梯度，并对其进行控制\n",
    "        grads = tf.gradients(cost / tf.to_float(batch_size), trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)\n",
    "        # 利用梯度下降优化算法进行优化.学习率为1.0\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n",
    "        # 相当于minimize的第二步，正常来讲所得到的list[grads,vars]由compute_gradients得到，返回的是执行对应变量的更新梯度操作的op\n",
    "        train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        return cost_per_token, train_op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
